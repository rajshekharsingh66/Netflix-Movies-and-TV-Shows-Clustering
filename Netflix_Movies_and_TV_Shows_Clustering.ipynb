{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajshekharsingh66/Netflix-Movies-and-TV-Shows-Clustering/blob/main/Netflix_Movies_and_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rajshekharsingh66/Netflix-Movies-and-TV-Shows-Clustering.git"
      ],
      "metadata": {
        "id": "LxeCpeOlYZRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member**    - Raj shekhar singh"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset comprises TV shows and movies available on Netflix as of 2019, sourced from Flixable, a third-party Netflix search engine. In 2018, Flixable released a report revealing a nearly threefold increase in the number of TV shows on Netflix since 2010, alongside a decrease of over 2,000 movies during the same period. The dataset provides an intriguing opportunity for exploration, with the goal of uncovering additional insights.\n",
        "\n",
        "The initial steps involve a thorough exploration of the dataset to gain a comprehensive understanding of its characteristics. Subsequently, data cleaning procedures are applied to ensure its readiness for analysis.\n",
        "\n",
        "The next phase involves preparing the dataset for clustering by implementing various parameters, such as removing stop words, white spaces, and numbers. This process aims to extract essential words that will serve as the basis for forming clusters.\n",
        "\n",
        "Following the data preparation, methods like the silhouette method and k-means elbow method are employed to identify the optimal number of clusters. The ultimate goal is to establish a recommender system using cosine similarity. This system will facilitate the recommendation of the top ten movies based on the clustered data.\n",
        "\n",
        "In summary, the approach involves dataset exploration, cleaning, preparation for clustering, cluster formation based on important words, determination of optimal clusters, and the creation of a recommender system using cosine similarity for suggesting the top ten movies.\n",
        "\n",
        "*   \n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub Link - https://github.com/rajshekharsingh66/Netflix-Movies-and-TV-Shows-Clustering/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset encompasses TV shows and movies available on Netflix as of 2019, sourced from Flixable, a third-party Netflix search engine.\n",
        "\n",
        "In 2018, Flixable released a compelling report revealing that the number of TV shows on Netflix had almost tripled since 2010, whereas the streaming service experienced a decline of over 2,000 movie titles during the same period. This notable shift in content composition raises the curiosity to delve deeper and extract additional insights from the dataset.\n",
        "\n",
        "Furthermore, combining this dataset with external sources such as IMDB ratings and Rotten Tomatoes could yield fascinating findings. Integrating these external datasets can offer a more comprehensive perspective on the content available on Netflix, considering both its quantity and quality. Exploring correlations between Netflix offerings and ratings from renowned platforms can provide valuable information about user preferences and the overall landscape of streaming content.\n",
        "\n",
        "**In this project, required to do:**\n",
        "\n",
        "\n",
        "\n",
        "Clustering similar content by matching text-based features.\n",
        "\n",
        "*  Exploratory Data Analysis.\n",
        "*  Understanding what type content is available in different countries.\n",
        "\n",
        "*  Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "*  Clustering similar content by matching text-based features.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing Numpy & Pandas for data processing & data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Importing  tools for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing libraries for hypothesis testing\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats import t\n",
        "from scipy.stats import f\n",
        "from scipy.stats import ttest_ind\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Word Cloud library\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Library used for textual data preprocessing\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Library used for Clusters implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Library used for building recommendation system\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import *\n",
        "\n",
        "# Library used for ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/Netflix-Movies-and-TV-Shows-Clustering/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Number of rows are: \",df.shape[0])\n",
        "print(\"Number of columns are: \",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Checking information about the dataset using info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dup = df.duplicated().sum()\n",
        "print(f'number of duplicated rows are {dup}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "null_counts = df.isnull().sum()\n",
        "\n",
        "# Plotting a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=null_counts.index, y=null_counts.values, palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.title('Null Values by Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Null Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Netflix dataset comprises information on TV shows and movies available on the platform as of 2019, encompassing 7787 entries and 12 columns. Null values are observed in the 'director,' 'cast,' 'country,' 'date_added,' and 'rating' columns. Given that there are only a few null values in the 'date_added' (10 instances) and 'rating' (7 instances) columns, these rows will be excluded from the dataset. Additionally, there are no duplicate values in the dataset."
      ],
      "metadata": {
        "id": "XuQ3CAZjhjuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include= 'all').T.round(2)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset contains movies and tv shows information (show id, type, title, director, release year, rating, duration etc.).**\n",
        "\n",
        "**Attribute Information** :\n",
        "\n",
        "**show_id**: Unique Id number for all the listed rows\n",
        "\n",
        "**type**: denotes type of show namely TV Show or Movie\n",
        "\n",
        "**title**: title of the movie\n",
        "\n",
        "**director**: Name of director/directors\n",
        "\n",
        "**cast**: lists the cast of the movie\n",
        "\n",
        "**country**: country of the production house\n",
        "\n",
        "**date_added**: the date the show was added\n",
        "\n",
        "**release_year**: year of the release of the show\n",
        "\n",
        "**rating**: show ratings\n",
        "\n",
        "**duration**: duration of the show\n",
        "\n",
        "**listed_in**: the genre of the show\n",
        "\n",
        "**description**: summary/ description of the movie"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "data = df.copy()\n",
        "data['cast'] = data['cast'].fillna(value='Not available')\n",
        "data['country'] = data['country'].fillna(value='Not Known')\n",
        "data = data.dropna(subset=['date_added','rating'])\n",
        "data['director'] = data['director'].fillna(value='Unknown')\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the data analysis ready i have done the following:\n",
        "\n",
        "1.Filled missing values of cast with Not available.\n",
        "\n",
        "2.Filled missing values of country with Not Known.\n",
        "\n",
        "3.Dropped rows of date_added missing values.\n",
        "\n",
        "4.Dropped rows of ratings missing values.\n",
        "\n",
        "5.Dropped the entire column of director as it had much number of missing values."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Pie Chart Visualization Code For Movie vs TV Show Share\n",
        "spread = data['type'].value_counts()\n",
        "plt.rcParams['figure.figsize'] = (5,5)\n",
        "\n",
        "# Set Labels\n",
        "spread.plot(kind = 'pie', autopct='%1.2f%%', cmap='Set1')\n",
        "plt.title(f'Movie vs TV Show share')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Wherever different percentage comparison comes into action, pie chart is used frequently. So, i have used Pie Chart and which helped us to get the percentage comparison more clearly and precisely."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we got to know that the types of shows available in netflix is not even with high count for TV shows. 69.14% of the data belongs to movies and 30.86% of the data for TV shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution of categories in various columns helps identify patterns and target specific demographics or areas of focus. For example, businesses can develop tailored marketing campaigns based on the types of shows most watched by the audience."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Count Plot Visualization Code for Various Ratings of Shows\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x='rating', hue='type', data=data, palette=['#564d4d', '#db0000'])\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Counts of Various Ratings')\n",
        "plt.xlabel('Ratings')\n",
        "plt.xticks(rotation = 60)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()\n",
        "\n",
        "# Printing The Counts of Each Rating for Different Type Shows\n",
        "print('Each Rating Counts for Different Types of Shows:')\n",
        "print(data.groupby(['rating', 'type']).size())"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we can clearly see that the most of the ratings are given by TV-MA followed by TV-14 and the least ratings are given by NC-17."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each TV show and movie on Netflix is assigned a maturity rating to help members make informed choices for themselves and their children. Netflix determines maturity ratings by the frequency and impact of mature content in a TV show or movie. TV show ratings reflect the overall maturity level of the whole series.**"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Count Plot Visualization Code for Content Produced by Different Countries\n",
        "# Not Taking Unknown Countries\n",
        "country_df = data[data['country'] != 'Not Known']\n",
        "\n",
        "# Set Labels\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(y='country', hue='type', data=country_df, palette=['#564d4d', '#db0000'], order=country_df.country.value_counts().iloc[:10].index)\n",
        "plt.title('Top Ten Countries With Most Content')\n",
        "plt.ylabel('Country')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()\n",
        "\n",
        "# Printing The Counts of Different Shows for Top 10 Countries\n",
        "print('Number of Shows Produced by Top 10 Countries:')\n",
        "print(country_df.groupby(['type']).country.value_counts().groupby(level=0, group_keys=False).head(10))"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above count plot we found that the content belongs to United States alone is 2546 (Movie: 1847, TV Show: 699) and followed by India is 923 (Movie: 852, TV Show: 71)."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we got to know:\n",
        "\n",
        "1. The United States is a leading producer of both types of shows (Movies and TV Shows), this makes sense since Netflix is a US company.\n",
        "\n",
        "2. The influence of Bollywood in India explains the type of content available, and perhaps the main focus of this industry is Movies and not TV Shows.\n",
        "\n",
        "3. On the other hand, TV Shows are more frequent in South Korea, which explains the KDrama culture nowadays."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create New DataFrames for Movie and TV Show Release\n",
        "release_year_df = data[['type','release_year']]\n",
        "movie_year = release_year_df[release_year_df['type']=='Movie'].release_year.value_counts().to_frame().reset_index().rename(columns={'index':'year','release_year':'count'})\n",
        "\n",
        "show_year = release_year_df[release_year_df['type']=='TV Show'].release_year.value_counts().to_frame().reset_index().rename(columns={'index':'year','release_year':'count'})"
      ],
      "metadata": {
        "id": "28C9D7ZbXE4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Line Plot Visualization Code for Content Released Over The Years\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "sns.lineplot(data=movie_year, x='year', y='count', color = '#db0000')\n",
        "sns.lineplot(data=show_year, x='year', y='count', color = '#564d4d')\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Content Released Over The Years')\n",
        "plt.legend(['Movie','TV Show'])\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UOcU_vsTXIem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Printing The Counts of Different Shows Released for Top 10 Years\n",
        "print('Number of Shows Released in Each Year:')\n",
        "print(data.groupby(['type']).release_year.value_counts().groupby(level=0, group_keys=False).head(10))"
      ],
      "metadata": {
        "id": "6l9jG-EKXS68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To see how the different contents are released over the years i have used line plot here.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the content on netflix are of the release date from 2010 to 2020."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1. Growth in the number of movies on Netflix is much higher than tv shows.\n",
        "2. Most of the content available was released between 2010 and 2020.\n",
        "3. The highest number of movies got released in 2017 and 2018 and tv shows got released in 2019 and 2020.\n",
        "4. The line plot shows very few movies, and tv shows got released before the year 2010 and in 2021. It is due to very little data collected from the year 2021."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Month from date_added Column\n",
        "data['month_added'] = pd.DatetimeIndex(data['date_added']).month"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame To Store Month Values and Counts\n",
        "months_df = data.month_added.value_counts().to_frame().reset_index().rename(columns={'index':'month', 'month_added':'count'})"
      ],
      "metadata": {
        "id": "HKtXzKzaXzPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Count Plot Visualization Code for Month Wise Addition of Contents on Netflix\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax=sns.barplot(data=months_df, x='month', y='count', palette='Reds_r')\n",
        "\n",
        "# Set Labels\n",
        "plt.title('Month Wise Addition of Contents')\n",
        "plt.xlabel('Month')\n",
        "for i in ax.patches:\n",
        "    ax.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 6), textcoords = 'offset points')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G5f2iDExX1J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most of the shows are uploaded either by year ending or beginning."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "From the above insight we got to know:\n",
        "\n",
        "1. October, November, December, and January are months in which many tv shows and movies get uploaded to the platform.\n",
        "2. It might be due to the winter, as in these months people may stay at home and watch tv shows and movies in their free time."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Movie and Separating Values\n",
        "df_movies = data[data['type']=='Movie'].copy()\n",
        "df_movies.duration = df_movies.duration.str.replace(' min','').astype(int)"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Histogram Visualization Code for Duration Distribution of Netflix Movies\n",
        "plt.figure(figsize=(8,4), dpi=120)\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.histplot(df_movies.duration, color='#db0000')\n",
        "plt.xticks(np.arange(0,360,30))\n",
        "\n",
        "# Set Labels\n",
        "plt.title(\"Duration Distribution for Netflix Movies\")\n",
        "plt.ylabel(\"% of All Netflix Movies\", fontsize=9)\n",
        "plt.xlabel(\"Duration (minutes)\", fontsize=9)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jn8a3TmOYLjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the duration distributions for the netflix movies."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that most of the movies last for 90 to 120 minutes."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1. On netflix most of the movies last for 90 to 120 minutes.\n",
        "2. So for target audience, movies duration will be greater than minimum 90 minutes."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 7 WordCloud Plot Visualization Code for Most Used Words in Netflix Shows Title\n",
        "# Create a String to Store All The Words\n",
        "comment_words = ''\n",
        "\n",
        "# Remove The Stopwords\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# Iterate Through The Column\n",
        "for val in data.title:\n",
        "\n",
        "    # Typecaste Each Val to String\n",
        "    val = str(val)\n",
        "\n",
        "    # Split The Value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts Each Token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "# Set Parameters\n",
        "wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10,\n",
        "                max_words = 1000,\n",
        "                colormap = 'gist_heat_r').generate(comment_words)\n",
        "\n",
        "# Set Labels\n",
        "plt.figure(figsize = (6,6), facecolor = None)\n",
        "plt.title('Most Used Words In Shows Title', fontsize = 15, pad=20)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud graphic is a visual representation that supplements a section of text to help readers better understand an idea or approach a subject from a different angle. A word cloud shows off trends."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above word cloud plot, it is observed that most repeated words in title include Christmas, Love, World, Man, and Story."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1. Most repeated words in title include Christmas, Love, World, Man, and Story.\n",
        "2. We saw that most of the movies and tv shows got added during the winters, which tells why Christmas appeared many times in the titles."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Count Plot Visualization Code for Top 10 Genres on Netflix\n",
        "# Separating Genres\n",
        "genres = df.set_index('title').listed_in.str.split(', ', expand=True).stack().reset_index(level=1, drop=True);\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Genres\n",
        "plt.figure(figsize=(10,5))\n",
        "g = sns.countplot(y = genres, order=genres.value_counts().index[:10], palette = \"Reds_r\")\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that international movies is in top in terms of genre and followed by dramas and comedies."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1. In terms of genres, international movies takes the cake surprisingly followed by dramas and comedies.\n",
        "2. Even though the United States has the most content available, it looks like Netflix has decided to release a ton of international movies.\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Count Plot Visualization Code for Top 10 Directors on Netflix\n",
        "directors = data[data.director != 'Unknown'].set_index('title').director.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Directors\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(y = directors, order=directors.value_counts().index[:10], palette='Reds_r')\n",
        "plt.title('Top 10 Directors on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that the most popular director in netflix is Jan Sutar and followed by Ral Campos and Marcus Raboy."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "1. Jan Suter, Ral Campos, Marcus Raboy, Jay Karas, Cathy Garcia-Molina, Jay Chapman are the top 5 directors which highest number of movies and tv shows are available in netflix.\n",
        "2. As we stated previously regarding the top genres, it's no surprise that the most popular directors on Netflix with the most titles are mainly international as well."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 Count Plot Visualization Code for Top 10 Actors on Netflix\n",
        "actor = data[data.cast != 'Not available'].set_index('title').cast.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# Set Labels and Ploting Graph for Top 10 Actors\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(y = actor, order=actor.value_counts().index[:10], palette='Reds_r')\n",
        "plt.title('Top 10 Actors on Netflix')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph, it is observed that most popular actors with most content in netflix are Anupam Kher, Shah Rukh Khan, Naseeruddin Shah and followed by Om Puri and Takahiro Sakurai."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above insight we got to know:\n",
        "\n",
        "That the actors in the top ten list of most numbers tv shows and movies are from India.\n",
        "Anupam Kher and Shah Rukh Khan have 30 above content alone in netflix."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Temporary Required month_added Column First\n",
        "data = data.drop(['month_added'], axis=1)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap Visualization Code\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# Plot Heatmap\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='Reds_r')\n",
        "\n",
        "# Setting Labels\n",
        "plt.title('Correlation Matrix heatmap')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XgiY7NQJcA8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is only one value in dataframe of int type, we are unable to visualize the Correlation Matrix heatmap."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot Visualization Code\n",
        "sns.pairplot(data, diag_kind=\"kde\", kind = 'reg')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and relationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is only one value in dataframe of integer type, we are unable to visualize the pair plot."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on above chart experiments i have noticed that some variable of our netflix dataset does not seems to normally distributed so i have made hypothetical assumption that our data is normally distributed and for that i have decided to do statistical analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Average number of movies on Netflix in **United States** is greater than the average number of movies on Netflix in **India**.\n",
        "2. The number of **movies** available on Netflix is greater than the number of **TV shows** available on Netflix.\n",
        "\n"
      ],
      "metadata": {
        "id": "fIPd7SpLJfJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average number of movies on Netflix in **United States** is greater than the average number of movies on Netflix in **India**."
      ],
      "metadata": {
        "id": "wrhFg3dXJt0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis:\n",
        "\n",
        "Alternate hypothesis:\n",
        "\n",
        "Test Type: Two-sample t-test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Split the data into the 'united states' and 'india's' movie produced groups\n",
        "us_movie_df = df_movies[df_movies.country == 'United States']\n",
        "india_movie_df = df_movies[df_movies.country == 'India']"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the two-sample t-test between the release years of the two groups of movies\n",
        "import scipy\n",
        "t_stat, p_val = scipy.stats.ttest_ind(us_movie_df['release_year'], india_movie_df['release_year'], equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in average number of movies produced by the 'United States' and 'India'.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant diff\")"
      ],
      "metadata": {
        "id": "GLlUPDwvJ9Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies available on Netflix in the United States and India, I conducted a two-sample t-test, also known as an independent samples t-test or unpaired t-test. I utilized the ttest_ind function from the scipy.stats module to carry out the test, which is suitable for analyzing the means of two independent samples. By applying this test, I was able to calculate the p-value and determine if there is a significant difference in the number of movies between the two countries."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the two-sample t-test for this analysis as it is suitable for comparing the means of two independent samples. In this case, we have two separate sets of movies data from Netflix for the United States and India, and we aim to determine if there is a significant difference in the average number of movies between these two countries."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of **movies** available on Netflix is greater than the number of **TV shows** available on Netflix."
      ],
      "metadata": {
        "id": "kdpIuNcpKHlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis:\n",
        "\n",
        "Alternate hypothesis:\n",
        "\n",
        "Test Type: Two sample z-test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Count the number of movies and TV shows in the DataFrame\n",
        "n_movies = data[data['type'] == 'Movie'].count()['type']\n",
        "n_tv_shows = data[data['type'] == 'TV Show'].count()['type']"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the counts and sample sizes for the z-test\n",
        "counts = [n_movies, n_tv_shows]  # Number of movies and TV shows\n",
        "nobs = [len(data), len(data)]  # Total number of observations in the DataFrame\n",
        "\n",
        "# Perform a two sample z-test assuming equal proportions\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
        "\n",
        "# Print the results\n",
        "print('Z-statistic:', z_stat)\n",
        "print('P-value:', p_val)\n",
        "print()\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in number of 'movies' and 'TV shows' available on Netflix.\")\n",
        "else:\n",
        "  print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant diff\")"
      ],
      "metadata": {
        "id": "0fOVbFbIKRYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the number of movies and TV shows available on Netflix, I conducted a two-sample z-test for proportions to obtain the p-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the two-sample z-test for proportions to compare the number of movies and TV shows available on Netflix because the data consists of two categorical variables."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its already handled in data wrangling, so now there are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Most of the columns are categorical, so no outliers observed)\n",
        "\n"
      ],
      "metadata": {
        "id": "dOwlOvgGJMsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(No need as the data is categorical)"
      ],
      "metadata": {
        "id": "mk86dk-JJQ7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Create a new column called 'tags' in the DataFrame 'data'\n",
        "# The purpose of this column is to store text data that will be used for model building\n",
        "# The text data consists of the 'description', 'rating', 'country', 'listed_in' and 'cast' columns\n",
        "data['tags'] = data['description'] + ' ' + data['rating'] + ' ' + data['country'] + ' ' + data['listed_in'] + ' ' + data['cast']\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Define a function to convert text into lower cases\n",
        "def to_lower(x):\n",
        "  return x.lower()\n",
        "\n",
        "# Apply the to_lower() function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(to_lower)\n",
        "\n",
        "# Cross checking our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Define a function to remove punctuations from text\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    # Replace each punctuation mark with no space, effectively deleting it from the text\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punct = text.translate(translator)\n",
        "    return text_without_punct\n",
        "\n",
        "# Apply the remove_punctuation function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_punctuation)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# 'tags' column does not have any URLs so remove words and digits containing digits\n",
        "data['tags'] = data['tags'].str.replace(r'\\w*\\d\\w*', '', regex=True)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Since the language is english, we need to import english stop words\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stop_words(x):\n",
        "  ''' function to remove stop words'''\n",
        "  x = x.split()\n",
        "  res = ''\n",
        "  for word in x:\n",
        "    if word not in stop_words:\n",
        "      res = res + ' ' + word\n",
        "  return res\n",
        "\n",
        "# Apply the remove_stop_words function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(remove_stop_words)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces in 'tags' column\n",
        "data['tags'] = data['tags'].str.strip()\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Rephrasing can be achived by the following code\n",
        "# def rephrase_tags(x):\n",
        "#     return x.replace('interesting', 'fascinating')\n",
        "# data['tags'] = data['tags'].apply(rephrase_tags)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Loading Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Apply the tokenization to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Cross-check our result that the function worked as expected\n",
        "print(data['tags'][0])\n",
        "\n",
        "# Store this list form of 'tags' column as 'temp_tags' for later POS tagging purpose\n",
        "temp_tags = data['tags']"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Define a function to Normalize Text function\n",
        "def stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Apply the stemming function to the 'tags' column of the DataFrame\n",
        "data['tags'] = data['tags'].apply(stemming)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i use Stemming.\n",
        "\n",
        "Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\". Stemming is important in natural language understanding (NLU) and natural language processing (NLP). Stemming is important in natural language processing(NLP). Nil means the suffix is replaced with nothing and is just removed. There may be cases where these rules vary depending on the words. As in the case of the suffix 'ed' if the words are 'cared' and 'bumped' they will be stemmed as 'care' and 'bump'.\n",
        "\n",
        "SnowballStemmer:\n",
        "\n",
        "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it. It was originally designed and built by Martin Porter. SnowballStemmer() is a module in NLTK that implements the Snowball stemming technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# Loading Libraries\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Apply the pos tagging to the 'tags' column of the DataFrame\n",
        "data['tags'] = temp_tags.apply(nltk.pos_tag)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "ptq4X5goF-_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function which gives true word (appropriate word) after pos tagging\n",
        "def sentence(data):\n",
        "  x=\"\"\n",
        "  for i in data:\n",
        "    a=i[0]+' '\n",
        "    x=x+a\n",
        "  return x\n",
        "\n",
        "# Apply the sentence function to the 'tags' column of the DataFrame\n",
        "data['tags']=data['tags'].apply(sentence)\n",
        "\n",
        "# Cross-check our result for the function created\n",
        "print(data['tags'][0])"
      ],
      "metadata": {
        "id": "zEpga_b0IHVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Create the object of tfid vectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', lowercase=False, max_features = 9000)\n",
        "# setting max features = 9000 to prevent system from crashing\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfidf.fit(data['tags'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfidf.vocabulary_.items()\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vector into array form for clustering\n",
        "vector = tfidf.transform(data['tags']).toarray()\n",
        "\n",
        "# Summarize encoded vector\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "gdIzy-JXHafy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.get_feature_names_out())"
      ],
      "metadata": {
        "id": "tpXa8nV-IRsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_data=pd.DataFrame(vector)\n",
        "vec_data"
      ],
      "metadata": {
        "id": "c-8u4AYiITzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've employed the TF-IDF (Term Frequency-Inverse Document Frequency) technique for text vectorization. TF-IDF, standing for term frequency-inverse document frequency, is a metric utilized in information retrieval and machine learning. It gauges the significance or relevance of textual elements (words, phrases, lemmas, etc.) within a document compared to a collection of documents, often referred to as a corpus.\n",
        "\n",
        "The reason for opting for TF-IDF lies in its superiority over Count Vectorizers. Unlike methods solely based on word frequency, TF-IDF assigns importance scores to terms. This not only captures the frequency of words in the corpus but also evaluates their significance. By doing so, I can eliminate less important words during analysis, simplifying model building by reducing input dimensions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes its needed, because dimensionality reduction removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. Its also helps to mitigate overfitting."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "# Using PCA to reduce dimensionality, this might take a while..\n",
        "pca = PCA(random_state=32)\n",
        "pca.fit(vector)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a Graph for PCA\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Set labels\n",
        "plt.title('PCA - cumulative explained variance vs number of components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.axhline(y= 0.8, color='red', linestyle='--')\n",
        "plt.axvline(x= 2500, color='green', linestyle='--')\n",
        "\n",
        "# Display chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7erOwxDeWRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the dimensions to 2500 using pca\n",
        "pca = PCA(n_components=2500, random_state=32)\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "_45eqZ0git3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformed features\n",
        "X = pca.transform(vector)"
      ],
      "metadata": {
        "id": "Tb0AKzL_ixBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use PCA to reduce the dimensionality of data.\n",
        "\n",
        "Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, we can start with PCA in order to visualize the relationship between points, to understand the main variance in the data, and to understand the intrinsic dimensionality.\n",
        "\n",
        "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Not required)"
      ],
      "metadata": {
        "id": "wptHCkWJeBv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Not required)"
      ],
      "metadata": {
        "id": "12Cvb0LeeGxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Finding optimal number of clusters using the elbow method\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "model = KMeans()\n",
        "visualizer = KElbowVisualizer(model, k=(3,12), metric='distortion', timings=False, locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(X)\n"
      ],
      "metadata": {
        "id": "7YKPrxf3jDHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this plot, we can say that the best k value is 6. Because, after this point the distortion/inertia is start decreasing in a linear fashion.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hqm4cuH9qBxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the K-Means clustering model where number of clusters is 6\n",
        "kmean=KMeans(n_clusters=6)\n",
        "\n",
        "# Fit the data to the KMean cluster\n",
        "kmean.fit(X)\n",
        "\n",
        "# Predict on the model\n",
        "y_kmean=kmean.predict(X)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the K-Means clustering model where number of clusters is 6\n",
        "kmean=KMeans(n_clusters=6)\n",
        "\n",
        "# Fit the data to the KMean cluster\n",
        "kmean.fit(X)\n",
        "\n",
        "# Predict on the model\n",
        "y_kmean=kmean.predict(X)"
      ],
      "metadata": {
        "id": "JmTUyTrcp9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column 'K_mean_cluster' in the dataset\n",
        "data[\"K_mean_cluster\"]=y_kmean\n",
        "data.head()"
      ],
      "metadata": {
        "id": "_N0XwaT5qI0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting unique labels\n",
        "u_labels = np.unique(y_kmean)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[y_kmean == i , 0] ,X[y_kmean == i , 1] , label = i)\n",
        "plt.title('Clusters for K-Means Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLR0GW9JqL2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Using the dendrogram to find the optimal number of clusters\n",
        "\n",
        "# Instantiate the dendogram\n",
        "plt.figure(figsize=(13,6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "\n",
        "# Set labels\n",
        "plt.title('Dendrogram')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=5.8, color='r', linestyle='--')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mA0A5-jcqhSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph we can say that optimal number of clusters is 6."
      ],
      "metadata": {
        "id": "d4k3ayzPqnN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Agglomerative clustering model where number of clusters is 6\n",
        "aggh = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
        "\n",
        "# Fit the data to the Agglomerative cluster\n",
        "aggh.fit(X)\n",
        "\n",
        "# Predict on the model\n",
        "y_hc=aggh.fit_predict(X)"
      ],
      "metadata": {
        "id": "25rs0gNqqpnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column 'Agg_cluster' in the dataset\n",
        "data[\"Agg_cluster\"]=y_hc\n",
        "data.head()"
      ],
      "metadata": {
        "id": "UCpkpkywqst3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting unique labels\n",
        "u_labels = np.unique(y_hc)\n",
        "\n",
        "# Plotting the results:\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[y_hc == i , 0] ,X[y_hc == i , 1] , label = i)\n",
        "plt.title('Clusters for Agglomerative Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLtIV2v1qxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 3 Implementation\n",
        "# Finding optimal number of clusters using the Silhouette Score\n",
        "for n_clusters in range(2,15):\n",
        "  km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "  km.fit(X)\n",
        "  preds = km.predict(X)\n",
        "  centers = km.cluster_centers_\n",
        "  score = silhouette_score(X, preds, metric='euclidean')\n",
        "  print (\"For n_clusters = %d, silhouette score is %0.4f\"%(n_clusters, score))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this chart we can say that optimal number of cluster is 5. Because the silhouette score is highest for the cluster 5."
      ],
      "metadata": {
        "id": "0x7-4iSzuGQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Silhouette Plots for Each Clusters\n",
        "# Instantiate the clustering model and visualizer\n",
        "for n_clusters in range(2,15):\n",
        "    km = KMeans (n_clusters=n_clusters, init ='k-means++', random_state=51)\n",
        "    km.fit(X)\n",
        "    preds = km.predict(X)\n",
        "    centers = km.cluster_centers_\n",
        "\n",
        "    # Set parameters and labels\n",
        "    score = silhouette_score(X, preds, metric='euclidean')\n",
        "    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(km)\n",
        "\n",
        "    visualizer.fit(X) # Fit the training data to the visualizer\n",
        "    visualizer.poof() # Draw/show/poof the data\n"
      ],
      "metadata": {
        "id": "eNl4PtenuLEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count Plot Visualization Code for number of movies and tv shows in each cluster\n",
        "# Set labels\n",
        "plt.figure(figsize=(12,6))\n",
        "graph = sns.countplot(x='K_mean_cluster',data=data, hue='type', palette=['#564d4d', '#db0000'])\n",
        "plt.title('Number of Movies and TV shows in each cluster - K-Means Clustering')\n",
        "plt.xlabel('Kmeans Clusters')\n",
        "\n",
        "# Adding value count on the top of bar\n",
        "for p in graph.patches:\n",
        "   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()), xytext = (0,3), textcoords = 'offset points')"
      ],
      "metadata": {
        "id": "z0Cpn7tC8hUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud Plot Visualization Code for User Rating Review\n",
        "# Define a Function for Clustering Similar Content by Matching Text-Based Features\n",
        "def kmeans_worldcloud(cluster_num):\n",
        "\n",
        " # Create a String to Store All The Words\n",
        "  comment_words = ''\n",
        "\n",
        "  # Remove The Stopwords\n",
        "  stopwords = set(STOPWORDS)\n",
        "\n",
        "  # Iterate Through The Column\n",
        "  for val in data[data['K_mean_cluster']==cluster_num].tags.values:\n",
        "\n",
        "      # Typecaste Each Val to String\n",
        "      val = str(val)\n",
        "\n",
        "      # Split The Value\n",
        "      tokens = val.split()\n",
        "\n",
        "      # Converts Each Token into lowercase\n",
        "      for i in range(len(tokens)):\n",
        "          tokens[i] = tokens[i].lower()\n",
        "\n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "  # Set Parameters\n",
        "  wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                  background_color ='white',\n",
        "                  stopwords = stopwords,\n",
        "                  min_font_size = 10,\n",
        "                  max_words = 1000,\n",
        "                  colormap = 'gist_heat_r').generate(comment_words)\n",
        "\n",
        "  # Set Labels\n",
        "  plt.figure(figsize = (6,6), facecolor = None)\n",
        "  plt.title(f'Most Important Words In Cluster {cluster_num}', fontsize = 15, pad=20)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "\n",
        "  # Display Chart\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "P6b1vmY88qLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 0\n",
        "kmeans_worldcloud(0)"
      ],
      "metadata": {
        "id": "_ea2BgAZ8tyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 1\n",
        "kmeans_worldcloud(1)"
      ],
      "metadata": {
        "id": "gJEvm-zJ8wiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 2\n",
        "kmeans_worldcloud(2)"
      ],
      "metadata": {
        "id": "oKJdyUDU8yCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 3\n",
        "kmeans_worldcloud(3)"
      ],
      "metadata": {
        "id": "HRFLO8WL82Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 4\n",
        "kmeans_worldcloud(4)"
      ],
      "metadata": {
        "id": "b6Y5Vfc_85Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for cluster 5\n",
        "kmeans_worldcloud(5)"
      ],
      "metadata": {
        "id": "WTsh8yfG9emD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here i have use topic modeling instead of feature importance and model explainability.\n",
        "# Model explainability does majory help in classification problem but here is the project of unsupervised ML.\n",
        "# In topic modeling, we can get topic wise feature importance."
      ],
      "metadata": {
        "id": "YawlDSOB9jwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i have use topic modeling. Assume that the clusters are topics. Here for topic modeling i use CountVectorizer process for Vectorization of data and i use Latent Dirichlet Allocation for building a topic."
      ],
      "metadata": {
        "id": "xG5bOGst9nB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use count vectorization process for our data\n",
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the count vectorizer using the text data\n",
        "document_term_matrix=count_vectorizer.fit_transform(data['tags'])"
      ],
      "metadata": {
        "id": "10HRuUAf9ov7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=6)\n",
        "lda.fit_transform(document_term_matrix)"
      ],
      "metadata": {
        "id": "n9LxaQ759q6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Most Important Features for Each Topic\n",
        "vocab = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(lda.components_):\n",
        "    vocab_comp = zip(vocab, comp)\n",
        "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_words:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "m_-lx8TB9v-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The project aimed to group television shows and movies by identifying both their commonalities and distinctions. The overarching objective was to construct a content-based recommender system capable of suggesting 10 shows to users based on their past viewing history. Noteworthy aspects of the project involve:\n",
        "\n",
        "\n",
        "*   Examining the dataset comprises 7787 entries and 12 attributes, emphasizing activities such as handling missing values and conducting exploratory data analysis (EDA).\n",
        "*  The examination indicated that Netflix possesses a higher quantity of movies compared to TV shows, and it is experiencing a notable expansion in its assortment of shows originating from the United States.\n",
        "\n",
        "\n",
        "*   For the purpose of clustering the shows, I opted for six essential attributes: director, cast, country, genre, rating, and description, all of which are categorical variables. These attributes underwent transformation through a 9000-feature TF-IDF vectorization, and the curse of dimensionality was addressed using Principal Component Analysis (PCA). By reducing the components to 2500, over 80% of the variance was successfully captured.\n",
        "*   Subsequently, I applied K-Means and Agglomerative clustering algorithms to categorize the shows. The elbow method validated that the most suitable number of clusters for K-Means was 6, while Silhouette score analysis suggested it to be 5.\n",
        "\n",
        "\n",
        "*  In Agglomerative clustering, the optimal cluster count was likewise determined to be 6, and this was visually represented using a dendrogram.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}